apiVersion: v1
kind: Template
metadata:
  name: k8s-custom-rules
  annotations:
    description: "Custom Rules for k8s"
objects:
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      prometheus: k8s
      role: alert-rules
    name: prometheus-k8s-custom-rules
    namespace: openshift-monitoring
  spec:
    groups:
    - name: custom_node_monitoring
      rules:
      - alert: node_ntp_timestamp
        expr: abs(node_time_seconds - timestamp(node_time_seconds))  > 1
        for: 10m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: NTP out of sync (current value is: {{$value}})"

      - alert: node_filesystem_free_dev_percent
        expr: node_filesystem_free_bytes / node_filesystem_size_bytes{device!~"/dev.*pv.*",device!~"/dev.*brick.*",device=~"/dev.*"} * 100 < 10
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Available Space on {{$labels.mountpoint}} (current value is: {{$value}})%"

      - alert: kube_node_status_limit_cpu_percent
        expr: sum(kube_pod_container_resource_limits_cpu_cores * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node) / sum(kube_node_status_allocatable_cpu_cores) BY (node) * 100 > 700
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: CPU Over-Allocation higher than {{$value}}%"

      - alert: kube_node_status_allocatable_cpu_percent
        expr: sum(kube_pod_container_resource_requests_cpu_cores * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node) / sum(kube_node_status_allocatable_cpu_cores) BY (node) * 100 > 98
        for: 30m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: More than 98% of all cores requested (current value is: {{$value}})%"

      - alert: kube_node_status_limit_mem_percent
        expr: sum(kube_pod_container_resource_limits_memory_bytes * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node) / sum(kube_node_status_allocatable_memory_bytes) BY (node) * 100 > 700
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: Memory Over-Allocation higher than {{$value}}%"

      - alert: kube_node_status_allocatable_mem_percent
        expr: sum(kube_pod_container_resource_requests_memory_bytes * on(pod, namespace) group_left kube_pod_status_phase{phase="Running"}) by (node)/ sum(kube_node_status_allocatable_memory_bytes) BY (node) * 100 > 98
        for: 30m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: More than 98% of memory requested (current value is: {{$value}})%"

      - alert: kube_persistentvolumeclaim_status
        expr: rate(kube_persistentvolumeclaim_status_phase{phase!="Bound"}[15m]) > 1
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.namespace}}: {{$labels.persistentvolumeclaim}} is {{$labels.phase}}"

      - alert: node_swap_enabled
        expr: node_memory_SwapTotal != 0
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "On {{$kubernetes_io_hostname}} swap enabled"

      - alert: node_filesystem_device_error
        expr: node_filesystem_device_error{fstype!="tmpfs",fstype!="fuse.glusterfs",mountpoint!~"/host/root/run/runc/.+"} > 0
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Errors on {{$labels.mountpoint}} detected"

      - alert: node_cpu_load5
        expr: max(node_load5)by (instance) / count(node_cpu{mode="idle"}) by (instance) > 2
        for: 30m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Load higher than (current value is: {{ $value }})"

      - alert: node_memory_free_percent
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Memory usage more than 90% (current value is: {{ $value }})%"

      - alert: node_procs_running
        expr: max(node_procs_running) by (instance)  / count(node_cpu{mode="idle"}) by (instance) > 100
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: More than 100 active processes running (current value is: {{ $value }})"

      - alert: node_procs_blocked
        expr: rate(node_procs_blocked[2m]) > 10
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: {{ $value }} processes blocked waiting for I/O to complete"

    - name: custom_kubernetes_monitoring
      rules:
      - alert: docker_pool_usage
        expr: max(container_fs_usage_bytes{device="vg_docker-docker--pool",job="kubernetes-cadvisor",id="/"}) by (instance)/ max(container_fs_limit_bytes{device="vg_docker-docker--pool",id="/"}) by (instance)*100 > 90
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Docker pool usage on {{$labels.instance}}: {{ $value }}%"

      - alert: kube_docker_operations_errors
        expr: rate(kubelet_docker_operations_errors[5m]) > 3
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Docker operation error: {{$labels.operation_type}} on {{$labels.instance}} failed"

      - alert: kube_docker_operations_latency_seconds_container_stop_actions
        expr: kubelet_docker_operations_latency_microseconds{quantile="0.9", operation_type=~"stop_container"}/1000/1000 > 50
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

      - alert: kube_docker_operations_latency_seconds_container_start_create_inspect_actions
        expr: kubelet_docker_operations_latency_microseconds{quantile="0.9", operation_type!~"stop_container" ,operation_type=~".*container" }/1000/1000 > 10
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

      - alert: kube_docker_operations_latency_seconds_container_pull_image
        expr: kubelet_docker_operations_latency_microseconds{quantile="0.9", operation_type="pull_image"}/1000/1000 > 300
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

      - alert: kube_docker_operations_latency_seconds
        expr: kubelet_docker_operations_latency_microseconds{operation_type!="pull_image",operation_type!~".*container",operation_type!~".*logs",quantile="0.9"} / 1000 / 1000 > 30
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Docker container operation {{$labels.operation_type}} on {{$labels.instance}} took {{ $value }} Seconds"

      - alert: kube_node_status_DiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Node has DiskPressure"

      - alert: kube_node_status_MemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Node has MemoryPressure"

      - alert: kube_node_status_OutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: Node is OutOfDisk"

      - alert: kube_pod_container_status_waiting
        expr: rate(kube_pod_container_status_waiting[5m]) > 0
        for: 1d
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.container}}: Container has waiting status longer than 5min"

      - alert: kubelet_pleg_relist_latency_seconds
        expr: kubelet_pleg_relist_latency_microseconds{quantile="0.9"}/1000000 > 5
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: PLEG latency high. High PLEG latency is often related to disk I/O performance on the docker storage partition."

    - name: custom_etcd_monitoring
      rules:
      - alert: etcd_disk_wal_fsync_duration_seconds_sum
        expr: etcd_disk_wal_fsync_duration_seconds_sum > 100
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Etcd disk write-ahead-log latency more than 100 milliseconds"

      - alert: etcd_server_proposals_failed_total
        expr: rate(etcd_server_proposals_failed_total[5m]) > 0
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Etcd server proposal failed"

    - name: custom_router_monitoring
      rules:
      - alert: router_kube_pod_container_status_ready
        expr: kube_pod_container_status_ready{pod=~".*router.*",namespace="default"} != 1
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.pod}}: pod NotReady"

      - alert: router_reload_seconds
        expr: template_router_reload_seconds{quantile="0.9"} > 3
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Router reloading takes more than {{ $value }} seconds on {{$labels.instance}}"

      - alert: router_router_write_config_seconds
        expr: template_router_write_config_seconds{quantile="0.9"} > 3
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Router is writing to disk takes more than {{ $value }}  seconds on {{$labels.instance}}"

      - alert: router_http_request_duration_seconds
        expr: rate(http_request_duration_microseconds[10m])/1000000 > 0.5
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Router HTTP request latencies are higher than {{ $value }} seconds on {{$labels.instance}}"

      - alert: router_on_same_node
        expr: count(kube_pod_info{pod=~"router.*"})by (node) > 2
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.pod}}:on same node"

    - name: custom_logging_monitoring
      rules:
      - alert: logging_es_container_count
        expr: count(kube_pod_container_status_ready{pod=~"logging-es-data-master.*",container="elasticsearch"}) < 3
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Less than 3 logging-es pods running for 5min"

      - alert: logging_es_on_same_node
        expr: count(kube_pod_info{pod=~"logging-es.*"}) by (node)  > 1
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.pod}}: running on same node"

      - alert: logging_es_cluster_is_timedout
        expr: es_cluster_is_timedout_bool != 0
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: ElasticSearch cluster timeout"

      - alert: logging_es_cluster_status
        expr: es_cluster_status != 0
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: ElasticSearch cluster status not ready"

      - alert: logging_es_fs_used
        expr: es_fs_total_free_bytes/es_fs_total_total_bytes*100 < 10
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: More than 90% space used"

      - alert: logging_es_mem_used
        expr: es_os_mem_free_bytes/es_os_mem_total_bytes*100 > 90
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: More than 90% memory used"

      - alert: container_cpu_usage_seconds_over_5min_logging
        expr: sum(rate(container_cpu_usage_seconds_total{namespace="openshift-logging"}[5m])) by (pod_name, kubernetes_io_hostname) > 2
        for: 10m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "High CPU usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}}. Current value: {{ $value }}%"

      - alert: logging_es_os_load_average
        expr: es_os_load_average > 5
        for: 1h
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.node}}: High Load detected"

    - name: custom_registry_monitoring
      rules:
      - alert: registry_container_count
        expr: count(kube_pod_info{pod=~"docker-registry.*",pod!~".*-deploy"}) < 2
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Less than 2 docker-registry running for 5min"

      - alert: registry_on_same_node
        expr: count by(node) (kube_pod_info{pod=~"docker-registry.*",pod!~".*-deploy"}) > 1
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.pod_name}}: running on same node"

    - name: custom_container_monitoring
      rules:
      - alert: container_cpu_usage_seconds_over_5min_default
        expr: sum(rate(container_cpu_usage_seconds_total{namespace="default"}[5m])) by (pod_name, kubernetes_io_hostname) > 0.5
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "High CPU usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}}. Current value: {{ $value }}%"

      - alert: container_cpu_usage_seconds_over_5min_openshift
        expr: sum(rate(container_cpu_usage_seconds_total{namespace=~"openshift-.*", namespace!="openshift-logging"}[5m])) by (pod_name, kubernetes_io_hostname) > 1
        for: 10m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "High CPU usage in {{$labels.namespace}} of {{$labels.pod_name}} {{ $value }}"

      - alert: container_cpu_usage_seconds_over_5min_glusterfs
        expr: sum by(pod_name, kubernetes_io_hostname) (rate(container_cpu_usage_seconds_total{namespace="glusterfs", container_name="glusterfs"}[5m])) > 1.5
        for: 30m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "High CPU usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}}. Current value: {{ $value }}%.  Gluster Service likely not working correctly."

      - alert: container_memory_working_set_default_registry
        expr: sum(container_memory_working_set_bytes{namespace="default", container_name="registry"} / 1024^3 ) by (pod_name, kubernetes_io_hostname) > 1
        for: 10m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Memory usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}} over 1GiB for 10min"

      - alert: container_memory_working_set_default_router
        expr: sum(container_memory_working_set_bytes{namespace="default", container_name="router"} / 1024^3 ) by (pod_name, kubernetes_io_hostname) > 2
        for: 10m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Memory usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}} over 2GiB for 10min"

      - alert: container_memory_working_set_logging
        expr: sum(container_memory_working_set_bytes{namespace="logging"}) by (pod_name, namespace, kubernetes_io_hostname) / sum(label_join(label_join(kube_pod_container_resource_limits_memory_bytes{namespace="logging"}, "pod_name", "", "pod"), "kubernetes_io_hostname", "", "node")) by (pod_name, namespace, kubernetes_io_hostname) > 0.75
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Memory usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}} over 75% of limit for 5min"

      - alert: container_memory_working_set_openshift-metrics
        expr: sum(container_memory_working_set_bytes{namespace="openshift-metrics"}) by (pod_name, namespace, kubernetes_io_hostname) / sum(label_join(label_join(kube_pod_container_resource_limits_memory_bytes{namespace="openshift-metrics"}, "pod_name", "", "pod"), "kubernetes_io_hostname", "", "node")) by (pod_name, namespace, kubernetes_io_hostname) > 0.9
        for: 5m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Memory usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}} over 90% of limit for 5min"

      - alert: container_memory_working_set_glusterfs
        expr: sum(container_memory_working_set_bytes{namespace="glusterfs"}) by (pod_name, namespace, kubernetes_io_hostname) / sum(label_join(label_join(kube_pod_container_resource_limits_memory_bytes{namespace="glusterfs"}, "pod_name", "", "pod"), "kubernetes_io_hostname", "", "node")) by (pod_name, namespace, kubernetes_io_hostname) > 0.85
        for: 30m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Memory usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}} over 85% of limit for 30min"

      - alert: container_memory_working_set_openshift-infra
        expr: sum(container_memory_working_set_bytes{namespace="openshift-infra"}) by (pod_name, namespace, kubernetes_io_hostname) / sum(label_join(label_join(kube_pod_container_resource_limits_memory_bytes{namespace="openshift-infra"}, "pod_name", "", "pod"), "kubernetes_io_hostname", "", "node")) by (pod_name, namespace, kubernetes_io_hostname) > 0.8
        for: 30m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "Memory usage of {{$labels.pod_name}} in namespace {{$labels.namespace}} on {{$labels.kubernetes_io_hostname}} over 80% of limit for 30min"

      - alert: pod_restart_count_non_infrastructure
        expr: kube_pod_container_status_restarts_total{namespace!~"openshift-.*|logging.*"} - (kube_pod_container_status_restarts_total{namespace!~"openshift-.*|logging.*"} offset 10m) > 1
        for: 24h
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.pod_name}} in namespace {{$labels.namespace}} restarted moren than once every 10 minutes over the last 24h, constant restarting might affect other projects"

      - alert: pod_restart_count
        expr: max_over_time(kube_pod_container_status_restarts_total[1h]) - min_over_time(kube_pod_container_status_restarts_total[1h]) > 1
        for: 1h
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.pod_name}}: Restarting"

    - name: custom_http_monitoring
      rules:
      - alert: http_probe_dns_lookup_time_seconds
        expr: probe_dns_lookup_time_seconds > 10
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: processing dns lookups takes {{ $value }} seconds"

      - alert: http_duration_seconds_processing
        expr: http_request_duration_microseconds/1000/1000 > 10
        for: 15m
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}}: http checks takes {{ $value }} seconds"

      - alert: http_probe_ssl_earliest_cert_expiry_days
        expr: (probe_ssl_earliest_cert_expiry - time())/60/60/24 < 20
        annotations:
          severity: ${SEVERITY_LABEL}
          message: "{{$labels.instance}} SSL cert expires in {{ $value }} days"

parameters:
- description: Labels used for routing alerting
  name: SEVERITY_LABEL
